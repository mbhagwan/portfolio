{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, and Load Data using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read CSV, JSON, and XML file types.\n",
    "* Extract the required data from the different file types.\n",
    "* Transform data to the required format.\n",
    "* Save the transformed data in a ready-to-load format   \n",
    "  which can be loaded into an RDBMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setting paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ElementTree` function from the `xml.etree` library is used to parse the information from   \n",
    "an `.xml` file format. The `.csv` and `.json` file formats can be read using the `pandas` library.\n",
    "\n",
    "To call the correct function for data extraction, you need to access the file information using   \n",
    "the `glob` library.\n",
    "\n",
    "To log the information correctly, you need the date and time information at the point of logging   \n",
    "and require the `datetime` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob, xml, and datetime are inbuilt features of Python\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file paths\n",
    "log_file = 'log_file.txt'  # to store all logs\n",
    "target_file = 'transformed_data.csv'  # to store final output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from CSV file format\n",
    "def extract_from_csv(file_to_process):\n",
    "    dataframe = pd.read_csv(file_to_process)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to extract data from JSON file format. The 'lines=True' \n",
    "argument enables the file to be read on a line to line basis'''\n",
    "def extract_from_json(file_to_process):\n",
    "    dataframe = pd.read_json(file_to_process, lines=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract from an `XML` file, first parse the data from the file using the `ElementTree`   \n",
    "function. Then extract the relevant information from this data and append it to a pandas   \n",
    "dataframe. This data has headers for \"name\", \"height\", and \"weight\" for different persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from XML file format\n",
    "def extract_from_xml(file_to_process):\n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"])\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        name = person.find(\"name\").text\n",
    "        height = float(person.find(\"height\").text)\n",
    "        weight = float(person.find(\"weight\").text)\n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height,\"weight\":weight}])], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the relevant function to extract data, write a function which uses the `glob` library   \n",
    "to identify the filetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    extracted_data = pd.DataFrame(columns=['name', 'height', 'weight'])  # empty data frame to hold extracted data\n",
    "\n",
    "    # process all csv files\n",
    "    for csvfile in glob.glob(\"source/*.csv\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True)\n",
    "\n",
    "    # process all json files\n",
    "    for jsonfile in glob.glob(\"source/*.json\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True)\n",
    "\n",
    "    # process all xml files\n",
    "    for xmlfile in glob.glob(\"source/*.xml\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True)\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The height (in inches) and weight (in pounds) in the extracted data is required to be in meters   \n",
    "and kilograms respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the unit conversion for the two parameters\n",
    "def transform(data):\n",
    "    '''Convert inches to meters and round off to two decimals\n",
    "    1 inch is 0.0254 meters '''\n",
    "    data['height'] = round(data.height * 0.0254,2)\n",
    "\n",
    "    '''Convert pounds to kilograms and round off to two decimals\n",
    "    1 pound is 0.45359237 kilograms '''\n",
    "    data['weight'] = round(data.weight * 0.45359237,2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Loading and Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transformed data to a `CSV` file that you can use to load to a database.\n",
    "\n",
    "Write a function that accepts the transformed data as a dataframe and the `target_file` path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target_file, transformed_data):\n",
    "    transformed_data.to_csv(target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the logging operation to record the progress of the different operations, by   \n",
    "recording a message, along with its timestamp, in the `log_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to record the message and its timestamp\n",
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'  # Year-Monthname-Day-Hour:Minute:Second\n",
    "    now = datetime.now()  # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)  # convert timestamp to string format\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(timestamp + ', ' + message + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ETL operations and log progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test all functions and log the ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data\n",
      "     name  height  weight\n",
      "0    alex    1.67   51.25\n",
      "1    ajay    1.82   61.91\n",
      "2   alice    1.76   69.41\n",
      "3    ravi    1.73   64.56\n",
      "4     joe    1.72   65.45\n",
      "5    alex    1.67   51.25\n",
      "6    ajay    1.82   61.91\n",
      "7   alice    1.76   69.41\n",
      "8    ravi    1.73   64.56\n",
      "9     joe    1.72   65.45\n",
      "10   alex    1.67   51.25\n",
      "11   ajay    1.82   61.91\n",
      "12  alice    1.76   69.41\n",
      "13   ravi    1.73   64.56\n",
      "14    joe    1.72   65.45\n",
      "15   jack    1.74   55.93\n",
      "16    tom    1.77   64.18\n",
      "17  tracy    1.78   61.90\n",
      "18   john    1.72   50.97\n",
      "19   jack    1.74   55.93\n",
      "20    tom    1.77   64.18\n",
      "21  tracy    1.78   61.90\n",
      "22   john    1.72   50.97\n",
      "23   jack    1.74   55.93\n",
      "24    tom    1.77   64.18\n",
      "25  tracy    1.78   61.90\n",
      "26   john    1.72   50.97\n",
      "27  simon    1.72   50.97\n",
      "28  jacob    1.70   54.73\n",
      "29  cindy    1.69   57.81\n",
      "30   ivan    1.72   51.77\n",
      "31  simon    1.72   50.97\n",
      "32  jacob    1.70   54.73\n",
      "33  cindy    1.69   57.81\n",
      "34   ivan    1.72   51.77\n",
      "35  simon    1.72   50.97\n",
      "36  jacob    1.70   54.73\n",
      "37  cindy    1.69   57.81\n",
      "38   ivan    1.72   51.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrlbh\\AppData\\Local\\Temp\\ipykernel_1580\\1055280171.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True)\n",
      "C:\\Users\\mrlbh\\AppData\\Local\\Temp\\ipykernel_1580\\2894970490.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height,\"weight\":weight}])], ignore_index=True)\n",
      "C:\\Users\\mrlbh\\AppData\\Local\\Temp\\ipykernel_1580\\2894970490.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height,\"weight\":weight}])], ignore_index=True)\n",
      "C:\\Users\\mrlbh\\AppData\\Local\\Temp\\ipykernel_1580\\2894970490.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height,\"weight\":weight}])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Log the initialization of the ETL process\n",
    "log_progress('ETL Job Started')\n",
    "\n",
    "# Log the beginning of the Extraction process\n",
    "log_progress('Extract phase Started')\n",
    "extracted_data = extract()\n",
    "\n",
    "# Log the completion of the Extraction process\n",
    "log_progress('Extraction phase Ended')\n",
    "\n",
    "# Log the beginning of the Transformation process\n",
    "log_progress('Transform phase Started')\n",
    "transformed_data = transform(extracted_data)  \n",
    "print('Transformed Data')\n",
    "print(transformed_data)\n",
    "\n",
    "# Log the completion of the Transformation process\n",
    "log_progress('Transform phase Ended')\n",
    "\n",
    "# Log the beginning of the Loading process\n",
    "log_progress('Load phase Started')\n",
    "load_data(target_file, transformed_data)\n",
    "\n",
    "# Log the completion of the Loading process\n",
    "log_progress('Load phase Ended')\n",
    "\n",
    "# Log the completion of the ETL process\n",
    "log_progress('ETL Job Ended')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
